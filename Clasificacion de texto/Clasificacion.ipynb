{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de textos\n",
    "\n",
    "En esta clase, exploraremos datos de mensajes de texto y crearemos modelos para predecir si un mensaje es spam o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1\n",
    "¿Qué porcentaje de los documentos en `spam_data` son spam?\n",
    "\n",
    "*Esta función debe devolver un valor flotante, el valor porcentual (es decir, $ ratio * 100 $).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_uno():\n",
    "    spam_data['target'].mean()\n",
    "    spam=spam_data[spam_data['target'] != 0]\n",
    "    print(len(spam), len(spam_data))\n",
    "    return spam_data['target'].mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747 5572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_uno()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `count_vectorizer` con parámetros predeterminados.\n",
    "\n",
    "Luego, ajuste un modelo de clasificación Naive Bayes multinomial. Calcule medidas de exactitud, presición, recall y f1-score usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver las cuatro medidas de evaluación como una lista con los 4 valores en el orden solicitado cada valor como flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def respuesta_dos():\n",
    "    scores=[]\n",
    "    vect = CountVectorizer().fit(X_train)\n",
    "    X_train_vectorized=vect.transform(X_train)\n",
    "    \n",
    "    clf=MultinomialNB()\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    X_test_vectorized=vect.transform(X_test)\n",
    "    predictions = clf.predict(X_test_vectorized)\n",
    "    \n",
    "    scores.append(accuracy_score(y_test,predictions))\n",
    "    scores.append(precision_score(y_test,predictions))\n",
    "    scores.append(recall_score(y_test,predictions))\n",
    "    scores.append(f1_score(y_test,predictions))\n",
    "    \n",
    "    spam=predictions[predictions!= 0]\n",
    "    print('Predicciones Spam:',len(spam))\n",
    "    print('Porcentaje Spam:',len(spam)/len(predictions)*100)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(tn, fp, fn, tp)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones Spam: 184\n",
      "Porcentaje Spam: 13.208901651112706\n",
      "[[1193    3]\n",
      " [  16  181]]\n",
      "1193 3 16 181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9863603732950467,\n",
       " 0.9836956521739131,\n",
       " 0.9187817258883249,\n",
       " 0.9501312335958005]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_dos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `count_vectorizer` con parámetros predeterminados.\n",
    "\n",
    "Luego, ajuste un modelo de clasificación Naive Bayes multinomial con suavizado (smoothing) `alpha = 0.1`. Encuentre el área bajo la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def respuesta_tres():\n",
    "    vect = CountVectorizer().fit(X_train)\n",
    "    X_train_vectorized=vect.transform(X_train)\n",
    "    clf=MultinomialNB()\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    X_test_vectorized=vect.transform(X_test)\n",
    "    predictions = clf.predict(X_test_vectorized)\n",
    "    print(predictions)\n",
    "    return roc_auc_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9581366823421557"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_tres()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 4\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` utilizando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **3**.\n",
    "\n",
    "Luego, ajuste un modelo de clasificador Naive Bayes multinomial con suavizado (smoothing) `alfa = 0.1` y calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_cuatro():\n",
    "    vectorizer = TfidfVectorizer(min_df=0.3)\n",
    "    X_train_vect = vectorizer.fit_transform(X_train)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_vect, y_train)\n",
    "    X_test_vect = vectorizer.transform(X_test)\n",
    "    predictions = clf.predict(X_test_vect)\n",
    "    print(predictions)\n",
    "    return roc_auc_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_cuatro()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 5\n",
    "\n",
    "¿Cuál es la longitud promedio de los documentos (número de caracteres) para los documentos spam y no spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (longitud promedio no spam, longitud promedio de spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def respuesta_cinco():\n",
    "    spam=spam_data[spam_data['target'] != 0]\n",
    "    sumLon = 0\n",
    "    for row in spam.iterrows():\n",
    "        sumLon += len(row[1].array[0])\n",
    "    promSpam = sumLon/len(spam)\n",
    "    no_spam= spam_data[spam_data['target'] != 1]\n",
    "    sum1 = 0\n",
    "    for row in no_spam.iterrows():\n",
    "        sum1 += len(row[1].array[0])\n",
    "    promNoSpam = sum1/len(no_spam)\n",
    "    return (promNoSpam,promSpam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71.02362694300518, 138.8661311914324)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_cinco()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "The following function has been provided to help you combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 6\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5**.\n",
    "\n",
    "Usando esta matriz de término de documento y una característica adicional, **la longitud del documento (número de caracteres)**, ajustar a un modelo de Clasificación de Vector de Soporte con regularización `C = 10000`. Luego calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def aux(data):\n",
    "    longitud = [] \n",
    "    for row in data:\n",
    "        longitud.append(len(row))\n",
    "    return np.resize(np.array(longitud),(len(longitud),1))\n",
    "    \n",
    "\n",
    "def respuesta_seis():\n",
    "    long = aux(X_train)\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5)\n",
    "    X_train_vect = vectorizer.fit_transform(X_train)\n",
    "    X_train_array = X_train_vect.toarray()\n",
    "    X_train_array = np.concatenate((X_train_array,long),axis =1)\n",
    "    clf = SVC(C = 10000)\n",
    "    clf.fit(X_train_array,np.array(y_train))\n",
    "    print(clf)\n",
    "    long1= aux(X_test)\n",
    "    X_test_vect = vectorizer.transform(X_test)\n",
    "    X_test_arra= np.concatenate((X_test_vect.toarray(),long1), axis=1)\n",
    "    predictions = clf.predict(X_test_arra)\n",
    "    print(predictions)\n",
    "    return roc_auc_score(y_test,predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10000, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[0 0 0 ... 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9665870159414631"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_seis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 7\n",
    "\n",
    "¿Cuál es el número promedio de dígitos por documento para los documentos no spam y spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # dígitos no es spam, promedio # dígitos spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def respuesta_siete():\n",
    "    spam=spam_data[spam_data['target'] != 0]\n",
    "    sumLon = 0\n",
    "    for row in spam.iterrows():\n",
    "        sumLon += len(re.findall('\\d+',row[1].array[0]))\n",
    "    promSpam = sumLon/len(spam)\n",
    "    no_spam= spam_data[spam_data['target'] != 1]\n",
    "    sum1 = 0\n",
    "    for row in no_spam.iterrows():\n",
    "        sum1 += len(re.findall('\\d+',row[1].array[0]))\n",
    "    promNoSpam = sum1/len(no_spam)\n",
    "    return (promNoSpam,promSpam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2549222797927461, 4.449799196787149)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_siete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 8\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `TfidfVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5** y usando **n-gramas de palabras n = 1 a n = 3** (unigramas, bigramas y trigramas).\n",
    "\n",
    "Usando esta matriz de término-documento y las siguientes características adicionales:\n",
    "* la longitud del documento (número de caracteres)\n",
    "* **cantidad de dígitos por documento**\n",
    "\n",
    "Ajustar un modelo de Regresión logística con regularización `C = 100`. Luego calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "*Esta función debe devolver el AUC como un flotante.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def aux_digit(data):\n",
    "    longitud = [] \n",
    "    for row in data:\n",
    "        longitud.append(len(re.findall('\\d+',row)))\n",
    "    return np.resize(np.array(longitud),(len(longitud),1))\n",
    "\n",
    "def respuesta_ocho():\n",
    "    long = aux(X_train)\n",
    "    digit = aux_digit(X_train)\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5,ngram_range = (1,3))\n",
    "    X_train_vect = vectorizer.fit_transform(X_train)\n",
    "    X_train_array = X_train_vect.toarray()\n",
    "    X_train_array = np.concatenate((X_train_array,long,digit),axis =1)\n",
    "    clf = LogisticRegression(C =100)\n",
    "    clf.fit(X_train_array,np.array(y_train))\n",
    "    \n",
    "    \n",
    "    long1 = aux(X_test)\n",
    "    digit1 = aux_digit(X_test)\n",
    "    X_test_vect = vectorizer.transform(X_test)\n",
    "    X_test_arra= np.concatenate((X_test_vect.toarray(),long1,digit1), axis=1)\n",
    "    predictions = clf.predict(X_test_arra)\n",
    "    print(predictions)\n",
    "    return roc_auc_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablocg/mineria/lib64/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9670347860041084"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_ocho()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 9\n",
    "\n",
    "¿Cuál es el número promedio de caracteres que no son palabras (cualquier cosa que no sea una letra, un dígito o un guión bajo) por documento para los documentos que no son spam y spam?\n",
    "\n",
    "*Sugerencia: utilice las clases de caracteres `\\ w` y` \\ W`*\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # caracteres que no son palabras, no spam, promedio de # caracteres que no son palabras, spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_nueve():\n",
    "    spam=spam_data[spam_data['target'] != 0]\n",
    "    sumLon = 0\n",
    "    for row in spam.iterrows():\n",
    "        sumLon += len(re.findall('\\W',row[1].array[0]))\n",
    "    promSpam = sumLon/len(spam)\n",
    "    no_spam= spam_data[spam_data['target'] != 1]\n",
    "    sum1 = 0\n",
    "    for row in no_spam.iterrows():\n",
    "        sum1 += len(re.findall('\\W',row[1].array[0]))\n",
    "    promNoSpam = sum1/len(no_spam)\n",
    "    return (promNoSpam,promSpam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.29181347150259, 29.041499330655956)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta_nueve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 10\n",
    "\n",
    "Ajustar y transformar los datos de entrenamiento `X_train` usando un `CountVectorizer` ignorando los términos que tienen una frecuencia de documento estrictamente inferior a **5** y utilizando **caracteres n-grams desde n = 2 a n = 5.**\n",
    "\n",
    "Para decirle a `CountVectorizer` que use caracteres n-grams, pase en `analyzer = 'char_wb'` que crea caracteres n-gramas solo del texto dentro de los límites de las palabras. Esto debería hacer que el modelo sea más robusto a los errores ortográficos.\n",
    "\n",
    "Usando esta matriz término documento y las siguientes características adicionales:\n",
    "* la longitud del documento (número de caracteres)\n",
    "* cantidad de dígitos por documento\n",
    "* **cantidad de caracteres que no son palabras (cualquier cosa que no sea una letra, dígito o guión bajo).**\n",
    "\n",
    "Ajustar un modelo de Regresión logística con regularización `C = 100`. Luego calcule el área bajo de la curva (AUC) usando los datos de prueba transformados.\n",
    "\n",
    "También **encuentre los 10 coeficientes más pequeños y los 10 más grandes del modelo** y devuélvalos junto con el AUC en una tupla.\n",
    "\n",
    "La lista de los 10 coeficientes más pequeños debe ordenarse primero, la lista de los 10 coeficientes más grandes debe ordenarse primero.\n",
    "\n",
    "Las tres características que se agregaron a la matriz de términos del documento deben tener los siguientes nombres si aparecen en la lista de coeficientes:\n",
    "['longitud_doc', 'conteo_digito', 'caracteres_no_palabra']\n",
    "\n",
    "*Esta función debe devolver una tupla `(AUC como flotante, lista de coeficientes más pequeños, lista de coeficientes más grande)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_noDigit(data):\n",
    "    longitud = [] \n",
    "    for row in data:\n",
    "        longitud.append(len(re.findall('\\W',row)))\n",
    "    return np.resize(np.array(longitud),(len(longitud),1))\n",
    "\n",
    "def coef_list(data):\n",
    "    noDigit = data[0][data.shape[1]-1]\n",
    "    digit = data[0][data.shape[1]-2]\n",
    "    long = data[0][data.shape[1]-3]\n",
    "    mayores = np.sort(data[0])\n",
    "    mayores = np.resize(mayores,(1,len(mayores)))[:10]\n",
    "    menores = np.sort(data[0])[::-1]\n",
    "    menores = np.resize(menores,(1,len(menores)))[:10]\n",
    "    mayList = []\n",
    "    menList = []\n",
    "    for xs in range(10):\n",
    "        if mayores[0][xs] == noDigit:\n",
    "            mayList.append('caracteres_no_palabra')\n",
    "        elif menores[0][xs] == noDigit:\n",
    "            menList.append('caracteres_no_palabra')\n",
    "        elif menores[0][xs] == digit:\n",
    "            menList.append('conteo_digito')\n",
    "        elif mayores[0][xs] == digit:\n",
    "            mayList.append('conteo_digito')\n",
    "        elif mayores[0][xs] == long:\n",
    "            mayList.append('longitud_doc')\n",
    "        elif menores[0][xs] == long:\n",
    "            menList.append('longitud_doc')\n",
    "        else:\n",
    "            mayList.append(mayores[0][xs])\n",
    "            menList.append(menores[0][xs])\n",
    "            \n",
    "    return [mayList, menList]\n",
    "        \n",
    "\n",
    "def respuesta_10():\n",
    "    long = aux(X_train)\n",
    "    digit = aux_digit(X_train)\n",
    "    noDigit = aux_noDigit(X_train)\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_df=0.5,ngram_range = (2,5), analyzer = 'char_wb')\n",
    "    X_train_vect = vectorizer.fit_transform(X_train)\n",
    "    X_train_array = X_train_vect.toarray()\n",
    "    X_train_array = np.concatenate((X_train_array,long,digit,noDigit),axis =1)\n",
    "    clf = LogisticRegression(C =100)\n",
    "    clf.fit(X_train_array,np.array(y_train))\n",
    "    \n",
    "    coef = clf.coef_\n",
    "    coef_result = coef_list(coef)\n",
    "    \n",
    "    long1 = aux(X_test)\n",
    "    digit1 = aux_digit(X_test)\n",
    "    noDigit1 = aux_noDigit(X_test)\n",
    "    X_test_vect = vectorizer.transform(X_test)\n",
    "    X_test_arra= np.concatenate((X_test_vect.toarray(),long1,digit1,noDigit1), axis=1)\n",
    "    predictions = clf.predict(X_test_arra)\n",
    "    print(predictions)\n",
    "    return (roc_auc_score(y_test,predictions),coef_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablocg/mineria/lib64/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9767393002054223,\n",
       " [[-0.7106744179473831,\n",
       "   -0.701436758811417,\n",
       "   -0.6897274618348326,\n",
       "   -0.6687712546870493,\n",
       "   -0.6546083789986065,\n",
       "   -0.6298854665355934,\n",
       "   -0.6205310317469349,\n",
       "   -0.6179826796312491,\n",
       "   -0.5739241458151942],\n",
       "  ['conteo_digito',\n",
       "   1.2189316795990313,\n",
       "   1.1092778530264071,\n",
       "   0.7920345629839769,\n",
       "   0.7699337609976461,\n",
       "   0.6422208360796439,\n",
       "   0.6249347269033307,\n",
       "   0.6222148460708873,\n",
       "   0.6069824098821436,\n",
       "   0.6001398194432559]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "Pn19K",
   "launcher_item_id": "y1juS",
   "part_id": "ctlgo"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
